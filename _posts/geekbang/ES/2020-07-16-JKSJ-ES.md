---
layout:      post
classify:    "极客时间"
title:       "ElasticSearch"
subtitle:    "快速构建分布式搜索和分析引擎"
date:        2020-07-16
catalog:     true
header-img: "img/geekbang/geekbang-main.jpg"
header-img-credit: "@geekbang.org"
header-img-credit-href: "www.geekbang.org"
author:      "EidLeung"
tags:
    - 极客时间
    - ElasticSearch
---

<div align="center">
	<font size = 8><b>ElasticSearch</b></font>
</div>
<p align="right"><a href="#如何购买">购买专栏</a></p>
## 一、入门
#### 安装
1. 单机运行多实例集群
```
bin/elasticsearch -E node.name=es0 -E cluster.name=xyl -E path.data=es0_data -d
bin/elasticsearch -E node.name=es1 -E cluster.name=xyl -E path.data=es1_data -d
bin/elasticsearch -E node.name=es2 -E cluster.name=xyl -E path.data=es2_data -d
bin/elasticsearch -E node.name=es3 -E cluster.name=xyl -E path.data=es3_data -d
```
### 文档（Document）
- _index：文档所属的索引名
- _type：文档所属的类型名
- _id：文档唯一ID
- _source：文档的原始JSON数据
- _all：整合所有字段内容到该字段【已废除】
- _version：文档的版本信息
- _score：相关性打分

### 索引（Index）
- Mapping定义包含的文档的字段名和字段类型
- Setting定义不同的数据分布

### 类型（Type）
一个索引只能创建一个Type，也即_doc

### 对比
RDBMS|ES
-|-
Table|Index(Type)
Row|Document
Column|Filed
Schema|Mapping
SQL|DSL


### REST
- 查看索引相关信息：
```
GET /<index>
```
- 查看索引总文档数：
```
GET /<index>/_count
```
- 查看索引前10条文档，了解文档格式：
```
POST /<index>/_search
{
}
```
- 查看indices
```
GET /_cat/indices/kibana*?v&s=index
```
- 查看状态为green的indices
```
GET /_cat/indices/kibana*?v&health=green
```
- 按照文档个数排序
```
GET /_cat/indices/?v&s=docs.count:desc
```
- 查具体的字段
```
GET /_cat/indices/kibana*?pri&v&h=health,index,pri,rep,docs.count,mt
```
- 查看每一个索引使用的内存
```
GET /_cat/indices?v&h=i,tm&s=tm:desc
```


### 节点
```
-E node.name=节点名称
```
- Data节点：保存数据
- Coordination节点：接受用户请求，分发到相应节点并相应用户请求

节点类型|配置参数|默认值
-|-|-
master eligible|node.master|true
data|node.data|true
ingest|node.ingest|true
corrdinating only|无|每个节点默认都是corrdinating节点</br>默认其他类型全为false
machine learning|node.ml|true（需要enable X-pack）
### 集群
```
-E cluster.name=集群名称
```
### 分片
- 主分片：解决水平扩展问题
- 副本：解决可行性问题

### CRUD
- 创建
1. PUT需要自己指定ID
```
PUT /<index>/_create/<ID>
```
2. POST系统会自动生成ID
```
POST /<index>/_doc
```
- 读取
```
GET /<index>/_doc/<ID>
```

- Index  
如果文档不存在，就创建新的文档；否则，先删除旧的文档，再创建新的文档并让version+1
```
PUT /<index>/_doc/<ID>
{
    [新文档的内容]
}
```
- 更新  
不会删除旧文档，真正的更新文档
```
POST /<index>/_update/<ID>
{
    "doc" {
        [要更新的内容]
    }
}
```
- BulK API  
一次API中调用中，进行多次操作，每次操作都是独立的，前面操作的失败不会影响后面的操作
```
POST _buik
{[要进行的操作：指定index和id]}
{[上面操作需要的数据]}
{[要进行的操作：指定index和id]}
{[上面操作需要的数据]}
```
- 批量读取：_mget  
```
GET _mget
{
    "docs":[
        {[要读取的数据：指定index和id]},
        {[要读取的数据：指定index和id]}
    ]
}
```
- 批量查询：_msearch
```
POST /<index>/_msearch
{[要操作的Index，如果在请求行已经指定可以省略]}
{[查询的条件]}
{[要操作的Index，如果在请求行已经指定可以省略]}
{[查询的条件]}
```




### 分词器
#### Standard Analyzer
默认分词器：按词切分，小写处理
1. Tokenizer  
- Standard
2. Token filters  
- Standard  
- Lower Case
- Stop(默认关闭)

#### Simple Analyzer
按照非字母切分（非字母的都被去除），小写处理
1. Tokenizer
- Lower Case

#### Stop Analyzer
停用词（the，a，is）过滤，小写处理
1. Tokenizer  
- Lower Case
2. Token filters 
- Stop

#### Whitespace Analyzer
按照空格切分，不转小写
1. Tokenizer
- Whitespace

#### Keyword Analyzer
不分词，直接建输入当做输出
1. Tokenizer
- Keyword

#### Patter Analyzer
正则吧表达式，默认\W+（非字符分割）切分
1. Tokenizer  
- Patter
2. Token filters 
- Lower Case
- Stop

#### Language Analyzer
提供了30多种参见语言的分词器

#### Customer Analyzer
自定义分词器


### 中文分词
#### ICU Analyzer
```
elasticsearch-plugin install analysis-icu
```
1. Character Filters
- Normalization
2. Tokenizer
- ICU Tokenizer
3. Token Filters
- Normalization
- Folding
- Collation
- Transform

#### [IK](https://github.com/medcl/elasticsearch-analysis-ik)
支持自定义的词库，支持热更新分词词典

#### [THULAC](https://github.com/microbun/elasticsearch-thulac-plugin)
THU Lexucal analyzer for Chinese，清华大学自然语言处理和社会人文计算实验室的一套中文分词器

#### [HanLP](https://github.com/KennFalcon/elasticsearch-analysis-hanlp)
面向生产环境的自然语言处理工具包
1. hanlp：hanlp默认分词器
2. hanlp_standard：标注分词
3. hanlp_index：索引分词
4. hanlp_npl：NPL分词
5. hanlp_n_short：N-最短路分词
6. hanlp_dijkstra：最短路分词
7. hanlp_crf：CRF分词（已废弃）
8. hanlp_speed：极速词典分词

#### [pinyin](https://github.com/medcl/elasticsearch-analysis-pinyin)

#### _analyzer API
```
# 直接指定Analyser进行测试
GET /_analyze
{
    "analyzer":"[分词器类型]",
    "text": "[要分词的文本]"
}
# 指定索引和字段进行测试
POST /<index>/_analyze
{
    "field":"[字段]",
    "text":"[要分词的文本]"
}
# 自定义分词器进行测试
POST /_analyze
{
    "tokenizer":"[tokenizer]"
    "filter":["[filter1]","[filter2]"],
    "text":"[要分词的文本]"
}
```

### Search API
语法|范围
-|-
/_search|集群上素有的索引
/<index>/_search|指定的index
/<index1>,<index2>/_search|指定的index列表
/<index>*/_search|指定的index前缀
#### 相关性
- 查准率（Presision）：尽量可能返回较少的无关文档  
True Positive / 全部返回的结构
- 查全率（Recall）：尽量返回较多的项管文档  
True Positive / 所有应该返回的结果
- Ranking：是否能够按照相关度进行排序

#### URL Search
```
GET /<index>/_search?q=[要查询的语句，使用Query String Syntax]&df=[默认字段]&sort=[排序]&from=[分页开始]&size=[分页大小]&timeout=[超时时间]
{
    "profile": [true/false：是否可以查看如何被执行]
}
```
##### Query String Syntax
- 字段查询：指定要查询的字段
- 泛查询：不指定查询的字段，默认查询所有字段
- Term查询：字段间使用OR进行连接，前面的使用字段查询，后面的使用泛查询
- 分组查询：使用小括号，均使用字段查询，字段间使用OR进行连接
- Phrase查询：使用双引号，均使用字段查询，字段间使用AND进行连接并且还要保持顺序一致
- 布尔操作必须大写：`AND(&&)`，`OR(||)`，`NOT(!)`
- 分组：必须有`+`，不能有`-`
- 范围查询：闭区间`[** TO **]`，开区间`{** TO **}`
- 算法符号：`>`，`<`，`>=`，`<=`
- 通配符查询：`?`表示1个字符，`*`表示0个或多个字符
- 正则表达式：
- 模糊匹配与近似查询：单个单词表示可以错误的编辑距离，多个单词表示单词之间可以相隔的单词数

#### Request Body Search
```
GET/POST /<index>/_search
{
    "_source":["[过滤]"],
    "from":"[分页开始]",
    "size":"[分页大小]",
    "sort":"[排序]",
    "query": {
        [要查询的内容]
    },
    "profile": [true/false：是否可以查看如何被执行]
}
```
##### Match
```
GET /<index>/_doc/_search
{
    "query": {
        "match" :{
            "[字段名称]":"[单词列表，默认OR连接]"
        }
    }
}

GET /<index>/_doc/_search
{
    "query": {
        "match" :{
            "[字段名称]": {
                "query": "[单词列表]",
                "operator": "[操作：AND/OR"
            }
        }
    }
}
```
##### Match Phrase
```
GET /<index>/_doc/_search
{
    "query": {
        "match_phrase" :{
            "[字段名称]": {
                "query": "[单词列表，按顺序出现]",
                "slop": [应许的编辑距离]
            }
        }
    }
}
```
##### Query String
```
POST /<index>/_search
{
     "query": {
        "query_seting": {
            "default_field":"[查询字段]",
            "query":"[查询条件]"
        }
    },
}
```
##### Simple Query String
```
POST /<index>/_search
{
     "query": {
        "simple_query_seting": {
            "fields":["[查询字段列表]"],
            "query":"[查询条件]",
            "default_opeartor":"[条件]"
        }
    },
}
```
### Mapping
定义字段名称和类型，并设置是否能够被索引  
#### 字段类型
- 简单类型
1. Text / Keyword
2. Date
3. Integer / Floating
4. Boolean
5. IPv4 & IPv6
- 复杂类型  
对象类型 / 嵌套类型
- 特殊类型  
geo_point & geo_shape / percolator

#### Dynamic Mapping
- 类型自动识别
JSON类型|Elasticsearch类型
-|-
字符串|1. 匹配日期格式，设置为Date</br>2. 匹配数字，设置为float或long，（默认是关闭的）</br>3. 设置为Text并增加Keyword字段
布尔值|boolean
浮点数|float
整数|long
对象|Object
数组|由第一个非空数值的类型所决定
空置|忽略

- 设置Dynmic Mapping
```
PUT /<index>
{
    "mappings": {
        "_doc": {
            "dynamic": true/false/strict
        }
    }
}
```
设置|true|false|strict
-|-|-|-
文档可索引（可插入文档）|YES|YES</br>但是新增字段不能被索引/搜索|NO
字段可索引（可新增字段）|YES|NO|NO
Mapping可更新|YES|NO|NO

#### 自定义Mapping
```
PUT /<index>
{
    "mappings" :{
        [mapping定义]
    }
}
//更新mapping
PUT /<index>/_doc
{
    "mappings" :{
        [mapping定义]
    }
}
```
- index：控制当前字段是否可以被搜索
- index_options：Text默认是positions，其他是docs
1. docs：记录doc的ID
2. freqs：记录doc的ID和term frequencies
3. positions：记录doc的ID、term frequencies和term position
4. offsets：记录doc的ID、term frequencies、term position和character offects
- null_value：实现对null值的搜索，只有Keyword支持设置为null_value
```
PUT /<index>
{
    "mappings" :{
        "properties":{
            "<字段名>":{
                "type":"keyword",
                "null_value":"NULL"
            }
        }
    }
}
```
- copy_to：将字段的数值拷贝到目标字段，便于通过目标字段进行查找，目标字段不会出现在_source中
```
PUT /<index>
{
    "mappings" :{
        "properties":{
            "<字段名>":{
                "type":"text",
                "copy_to":"[目标字段]"
            }
        }
    }
}
```
### Analyzer
精确值：不需要进行分词
全文本：需要分词

##### Character Filters
在Tokenizer对文本进行处理
- 自带的Character Filters
1. HTML Strip：去掉html标签
2. Mapping：字符串替换
3. Pattern replace：正则匹配替换

##### Tokenizer
把文本切分成分词（trem or token）
- 自带的Tokenizer
1. whitespace：
2. standard：
3. uax_url_email：
4. pattern：
5. keyword：不做处理
6. path hierarchy：文件路径

##### Token Filters
对Tokenizer输出的单词（term）进行增加、修改、删除
- 自带的Token Filters
1. Lowercase：变为小写
2. stop：去除停用词
3. synonym：添加近义词

#### 自动以Analyzer
```
PUT /<index>
{
    "settings" {
        "analysis": {
            "analyzer": {
                "[自定义的analyzer的名称]": {
                    "type":"custom",
                    "char_filter":["[character filter列表]"],
                    "tokenizer":"[tokenizer名称]",
                    "filter":["[token filter列表]"]
                }
            },
            "tokenizer": {
                "[指定的tokenizer名称]": {
                    "type": "具体的tokenizer",
                    "[参数名]":"[参数值]"
                }
            },
            "char_filter": {
                "[指定的character filter名称]": {
                    "type": "具体的haracter filter",
                    "[参数名]":"[参数值]"
                }
            },
            "filter": {
                "[指定的token filter名称]": {
                    "type": "具体的token filter",
                    "[参数名]":"[参数值]"
                }
            }
        }
    }
}
```

#### Template
##### Index Template
1. 应用Elasticsearch默认的Setings和mappings
2. 应用order值较低的Index Template覆盖默认的
3. 应用order值较高的Index Template覆盖较低的
4. 使用自定义的Settings和Mappings覆盖模板中的

##### Dynamic Template
根据Elasticsearch识别的数据类型，结合字段的名称，来动态设定字段类型，如：
1. 所有得字符串类型设定成Keyword，或管理keyword字段
2. is开头的字段设置成boolean
3. long_开头的设置成long型
```
PUT /<index>
{
    "mappings": {
        "dynamic_emplates": [
            {
                "[规则名称]": {
                    "path_match":"[匹配规则]",
                    "path_unmatch":"[不匹配规则]",
                    "mapping": {
                        "type":"[要设置的字段类型]"
                    }
                }
            }
        ]
    }
}
```

### 聚合（Aggregation）
#### 分类
1. Bucket Aggregation：一些列满足特定条件的文档的聚合
```
GET /<index>/_search
{
    "size": [返回文档个个数],
    "aggs": {
        "[聚合名称]": {
            "terms" : {
                "field": "[字段名称]"
            }
        }
    }
}
```
2. Metric Aggregation：一些数学运算，可以对文档字段进行统计分析
```
GET /<index>/_search
{
    "size": [返回文档个个数],
    "aggs": {
        "[聚合名称]": {
            "[聚合操作：max/avg/min]" : {
                "field": "[字段名称]"
            }
        }
    }
}
```
3. Pipeline Aggregation：对其他的聚合记过进行二次聚合
4. Matrix Aggregation：支持对多个字段的操作并提供一个结果矩阵

## 二、搜索

### Term查询
Term是表达语义的最小单位，进行自然语言处理（搜索、统计）都需要处理Term
```
POST /<index>/_search
{
    "query": {
        "term": {
            "[字段名] / [字段名].keyword" : {
                "value": "[字段值]"
            }
        }
    }
}
```
#### 特点
1. Term查询包括，Trem Query / Range Query ? Exists Query / Prefix Query / widcard Query
2. 不会对Term做分词（查询输入不会分词，但是插入的时候要根据index的mapping而定）
3. 可以通过Constant Score将查询转换成一个Filtering，避免算分，并利用缓存，提高性能
```
POST /<index>/_search
{
    "query": {
        "constant_score": {
            "filter" :{
                "trem": {
                    "[字段名].keyword":"[字段值]"
                }
            }
        }
    }
}
```
### 全文查询
包含Match Query / Match Phrase Query / Query String Query
#### 特点
1. 会进行分词
2. 对每个分词进行查询，并合并结果、算分

### 结构化搜索
### 相关性
1. 词频：TF，Term Frequency
2. 逆文档频率：IDF，Inverse Document Frequency
3. 可以设置boost的值来影响算分

#### TF-IDF
`$\sum(TF*IDF)$`
#### BM25

### Query & Filter
Query 会进行算分；Filter不进行算法，可以利用Cache，提升性能

#### Bool Query
可以写多个条件子句，并进行组合，可嵌套
查询方式|匹配性质|贡献算分
-|-|-
must|必须匹配|YES
should|选择性匹配|YES
must_not|Filter Context 查询子句，必须不能匹配| - 
filter|Filter Context 必须匹配|NO
```
POST /<index>/_search
{
    "query": {
        "bool": {
            "must/should/must_not/filter" : {
                "term/range": {
                    "[字段名]": "[字段值/条件+值]"
                }
            }
        }
    }
}
```
#### Boosting Query
可以给字段指定加分还是减分，以影响结果的排序
```
POST /<index>/_search
{
    "query": {
        "boosting": {
            "positive / negative": {
                "match": {
                    "[字段名]": "[字段值]"
                }
            },
            "negative_boost": [要减的boost分]
        }
    }
}
```


#### Disjunction Max Query
将任何与任一查询匹配的文档都作为结果，并将字段上评分最高的评分作为最终评分返回  
- 可以增加tie_breaker(0,1)增加算分的准确度（可认为tie_breaker默认为0）
1. 获得最佳算分
2. 其他算分乘以tie_breaker
3. 求平均
```
POST /<index>/_search
{
    "query": {
        "dis_max": {
            "queries": {
                "match": {
                    "[字段名]": "[字段值]"
                },
                 "match": {
                    "[字段名]": "[字段值]"
                }
            },
            "tie_breaker": "[值]"
        }
    }
}
```
#### Multi Match
```
POST /<index>/_search
{
    "query": {
        "multi_match": {
            "type": "best_fields（默认） / most_fileds / cross_Fields",
            "fields": ["[字段列表]"],
            "query": "[字段值]",
            "tie_breaker": "[值]"
        }
    }
}
```
1. 最佳字段（Best Fields）：字段之间存在竞争，有相互关联，评分来自最匹配字段
2. 多数字段（Most Fields）：匹配字段越多则越好，不能使用opeartor指定操作类型
3. 混合字段（Cross Field）：单个字段只作为整体的一部分，希望在列出的字段总找到尽量多的词

### Search Template
可以使查询和优化分离
```
POST _scripts/<template_id>
{
    "script": {
        "lang": "mustcahe"
        "source":"定义的template"
    }
}

POST /<index>/_search/template
{
    "id":"[template_id]",
    "params": {
        "[参数名]":"[参数值]"
    }
}
```
### Index Alias
索引别名：类似于数据表的视图
```
POST _aliases
{
    "actions":[
        {
            "add": {
                "index": "[目标索引名]",
                "alias": "[别名]"
            }
        }
    ]
}
```
### Function Score Query
在查询后为，对每个匹配的文档进行重新计分，并根据新的积分排序，

- 默认计分函数
1. Weight：为每一个文档设置一个简单而不被规范化的权重  
`新的算法 = 老的算分 * 平滑函数(权重)`
2. Field Value Factor：使用该数字来修改_score，例如将“热度”和“点赞数”作为算分的参考因素
3. Random Score：为每一个用户使用一个不同的，随机算分结果
4. 衰减函数：以某个字段的值为标准，距离某个值越近，得分越高
5. Script Score：自定义脚本完全控制所需逻辑

#### 一致性随机函数
只要seed一致，算分将保持一致
```
POST /<inces>/_search
{
    "query": {
        "function_sccore": {
            "random_score": {
                "seed": [seed值]
            }
        }
    }
}
```
### Suggester API
```
POST /<index>/_search
{
    "query": {
        ……
    },
    "suggest": {
        "term-suggestion": {
            "text": "[查找的内容]",
            "term": {
                "suggest_mode": "missing / popular / always / ……",
                "field": "body"
            }
        }
    }
}
```
1. Term & Phrase Suggester
2. Complete & Context Suggester

#### Term Suggester

- Suggestion Mode
1. Missing: 如果索引已经存在，不提供建议
2. Popular：推荐出现频率更高的词
3. Always：总是提供建议

#### Phrase Suggester
在term suggester上增加了一些额外的逻辑和参数
- Suggestion Mode（增加的）
1. Max Errors：最多可以拼错的Terms数
2. Confidence：限制返回结果数，默认1

#### Complete Suggester
1. 需要对索引进行定义，使用对字段使用Completion type
```
PUT /<index>
{
    "mappings": {
        "properties": {
            "[字段名称]": {
                "type": "completion"
            }
        }
    }
}
```
2. 索引数据（写入数据）
3. 运行suggest查询
```
POST /<index>/_search?pretty
{
    "suggest": {
        "[suggestion名称，随意写]": {
            "prefix": "[前缀]",
            "completion": {
                "field": "[要查询的字段]"
            }
        }
    }
}
```
#### Context Suggester
能够实现基于Category（任意字符串）和Geo（地理位置信息）的Suggester
1. 定制一个Mapping
```
PUT /<index>
{
    "mappings": {
        "properties": {
            "type": "completion",
            "contexts": [{
                "type": "category / geo".
                "name": "[category名称]"
            }]
        }
    }
}
```
2. 索引数据（写入数据）
```
POST /<index>/_doc
{
    ……
    "[filed名称]": {
        "input": ["[field值]"],
        "contexts": {
            "[category名称]": "[category类型]"
        }
    }
}
```
3. 结合Context进行suggestion查询
```
POST /<index>/_search?pretty
{
    "suggest": {
        "[suggestion名称，随意写]": {
            "prefix": "[前缀]",
            "completion": {
                "field": "[要查询的字段]",
                "contexts": {
                    "[category名称]": "[category类型]"
                }
            }
        }
    }
}
```
#### 精准度与召回率
1. 精准度
Completion > Phrese > term
2. 召回率
Term > Phrease > Completion
3. 性能
Completion > Phrese > term

### 跨集群搜索
1. 配置
```
PUT _cluster/settings
{
    "persistent": {
        "cluster": {
            "remote": {
                "[集群2]": {
                    "seeds": ["IP:PORT列表"]
                },
                "[集群1]": {
                    "seeds": ["IP:PORT列表"]
                }
            }
        }
    }
}
```
2. 使用
```
GET /<claster:<index>/_search
{
    "query": {
        ……
    }
}
```
## 三、分布式
### Node
1. 不同的集群通过集群名字进行区分，可以通过cluster.name进行设定
2. 一个节点就是一个Elasticsearch的实例（Java进程），一台机器上可以有多个，通过node.name指定节点的名字，节点启动后，会分配一个UID，保存在data目录下

#### Coordinating Node
1. 将用户请求发放到相应的节点（如，创建索引的请求，需要发送到Master 节点）
2. 所有节点默认都是Coordinating Node，可以通过设置其他类型为false将集群设置成Dedicated Coordination Node（只用于协调的节点）

#### Data Node
1. 保存数据的节点，节点启动后默认就是数据节点，可以将node.data设置成false禁用
2. Data Node的职责是保存分片数据（Master Node会决定如何把分片发送到数据节点上）
3. 利用Data Node可以实现数据水平扩展并解决数据单点问题

#### Master Node
1. 创建、删除索引；决定分片分配到哪个节点
2. 维护并更新Cluster State

#### Master Eligible Node
1. 必要时参与Master的选举，并成为Master节点
2. 节点默认为Master Eligible Node，可用设置node.master为false禁止
3. 集群中第一个Master Eligible Node启动时，会选举自己为Master

#### 集群信息（Cluster State）
1. 所有节点信息
2. 索引和其相关的Mapping、Setting信息
3. 分片的路由信息
4. 每个节点都会保存，但是只有Master Node才能修改
5. 将集群discovery.zen.minimum_master_node设置为master节点总数/2+1避免脑裂（7.0以后不用设置）

### Shard
1. 默认主分片为1，副本分片为0
2. 主分片在索引创建时指定，后续不能更改
3. 文档通过`hash(_routing)%number_of_primary_shards`映射到分片
4. 可以能通过指定routing值来决定文档到分片的映射
```
PUT /<index>/_doc/<ID>?routing=[routing值]
{
    [doc内容]
}
```

#### Refresh & Transaction Log
1. 写入文档会先写入Index Buffer（大小为JVM的10%）
2. 同时也会写入Transaction Log（默认会落盘）
2. 每个1秒（通过index.refresh_interval配置）或Index Buffer满，会见Index Buffer写入磁盘（Segment），这里可以利用操作系统的文件缓存提升性能，即Refresh

#### Flush
1. 默认30分钟一次或者Transaction Log写满（默认大小为512M)
2. 调用Refresh，清空Index Buffer
3. 调用fsync，将缓存中的Segment写入磁盘
4. 清空（删除）Transaction Log

#### Merge
1. 定期将Segment合并，减少Segment的数量并删除已经删除的文档
2. 可以通过`POST /<index>/_forcemerge`API强制Merge

### 分布式搜索
#### Query
1. Coordinating节点收到用户请求后，随机选择分片发送查询请求
2. 收到查询请求的分片所在节点执行查询并排序返回`from+size`数据

#### Fetch
1. Coordinating将分片节点返回的结果进行合并排序并选取`from`到`from+size`文档ID
2. 以multi get的方式到分片所在节点获取文档详细信息

#### 注意
1. 每个分片要查询`from+size`个文档
2. Coordinating要处理`number_fo_shard*(from+size)`个数据
3. 深度分页

#### 解决分布式算分不准问题
1. 文档不多的时候，把主分片设置成1，文档多的时候，保证均匀分布在各个分片上
2. 使用DFS Query Then Fetch`_search?serach_type=dfs_query_then_fetch`，不过会更加耗CPU和内存，一般不建议使用

### 排序
1. 排序针对的是原始字段内容，因此倒排索引在此时无用
2. 此时需要正排索引，及通过文档的ID和字段快速得到原始内容
3. ES中使用Field Date和Doc Values（列式存储，对Text类型无效）实现正排索引
4. Doc Values默认是开启的，并在索引时，和倒排索引一起创建，也可以通过设置关闭,c重新打开需要重建索引（在不需要做排序和聚合分析的时候才关闭）
```
PUT /<index>/_mapping
{
    "properties": {
        "[字段名]": {
            "type": "keyword",
            "doc_values": false
        }
    }
}
```
### 分页和遍历
```
POST /<index>/_search
{
    "from": [开始],
    "size": [大小，默认10],
    "query": {
        ……
    }
}
```
####  使用Search After API避免深度分页
```
POST /<index>/_search
{
    "size": [大小，默认10],
    "query": {
        ……
    },
    "search_after": [上一次sort最后的的值],
    "sort": [
        {[需要的排序]},
        {"_id", "asc"}
    ]
}
```
1. 不支持指定from，只能往下翻页
2. 第一次搜索需要指定sort，并且保证值是惟一的（通常是将文档的_id放入）
3. 之后查询时将上一次返回结果的最后一条数据的sort值放入search_after中

#### Scroll API
第一次查询是生成快照，当有新数据写入时，新数据无法被查到
1. 在调用的第一次指定一个scroll存活的时间
```
POST /<index>/_search?scroll=5m
{
    ……
}
```
2. 之后每次一次查询输入上一次的scroll id
```
POST /_search/scroll
{
    "scroll": "5m",
    "scroll_id": "[上一次的scroll id]"
}
```
### 并发控制
1. 建议采用_seq_no和_primary_term的乐观锁控制
```
PUT /<index>/_doc/<ID>?if_seq_no=[*]&if_primary_term=[*]
{
    [新的内容]
}
```
2. 也可采用version进行，version的值必须大于当前ES中存储的version值
```
PUT /<index>/_doc/<ID>?version=[*]&version_type=external
{
    [新的内容]
}
```

## 四、聚合（Aggregation）
```
"aggs": {
    "<aggregation_name>": {
        "<aggregation_type>": {
            <aggregation_boy>
        },
        [, "meta": { [<meta_ata_boy] }]?
        [, "aggreagtions": [<sub_aggreagtion>]]?
    }
    [, "<aggregation_name_2>": {……} ]*
}
```
#### Metric Aggreagtion
```
"[aggregation_type]": {
    "field": "[field_name]"
}
```
1. 单值分析
- max、min、avg、sum
- Cardinality（类似于distinct count）

2. 多值分析
- stats、extened stats（统计，包含：count、min、max、avg、sum）
- percentile、percentile rank
- top hits（排在前面的示例）

#### Bucket Aggreagtion
分类统计（类似于group by），
1. terms（注意Text字段需要打开fielddata字段才能进行terms aggregation）
```
"terms": {
    "field": "[field_name]"
}
```
2. 数值类型
- range、date Range
```
"range": {
    "field": "[field_name]",
    "range": [{"from":"[开始值]", "to":"[结束值]"}]
}
```
- histogram、date histogram
```
"historgram": {
    "field": "[字段名]",
    "interval": "[步长]",
    "exterded_bounds": {
        "min": "[起始值]",
        "max": "[终止值]"
    }
}
```
3. 支持嵌套

#### Pipeline Aggregation
二次聚合分析
1. Sibling-分析结果与现有结果同级
```
"aggs": {……},
"[pipeline_aggregation_name]": {
    "pipeline_aggregation_type": {
        "bucket_path": "要处理的aggs的名称"
    }
}
```
2. Parent-分析结果内嵌到现有的聚合分析结果之中
```
"aggs": {
    "<aggs_name>":{……},
    "[pipeline_aggregation_name]": {
        "pipeline_aggregation_type": {
            "bucket_path": "要处理的aggs的名称"
        }
    }
}
```
#### 聚合的作用范围
- Filter
```
"aggs": {
    "<aggs_name>": {
        "filter": {……},
        "aggs": {……}
    }
}
```
- Post Filter
```
"aggs": {……},
 "post_filter": {……}
}
```
- Global  
忽略query的限定条件，对所有文档进行aggregation
```
"query: {……},
"aggs": {
    "<aggs_name>": {
        "global": {},
        "aggs": {……}
    }
}
```
#### 聚合额排序
```
"aggs": {
    "<aggs_name>": {
        "aggs_type": {
            "field": "[聚合字段]",
            "order": [
                {"[排序字段/ 聚合名]", "[排序规则]"}
            ]
        }
    }
}
```
## 五、数据建模
#### 嵌套对象
- 设置Mapping
```
PUT /<index>/
{
    "mappings": {
        "properties": {
            "<field_name>": {
                "type": "nested",
                "properties": {[字段信息]}
            }
        }
    }
}
```
- 进行查询
```
"query": {
    ……
    "nested": {
        "path": "[字段名]",
        "query": {……}
    }
}
```
- 聚合分析
```
"aggs": {
    "[aggs_name]": {
        "nestd": {
            "path": "[对象字段]"
        },
        "aggs": {……}
    }
}
```
#### 父子文档
- 设置Mapping
```
PUT /<index>/
{
    "mappings": {
        "properties": {
            "[关联名称]": {
                "type": "join",
                "relations": {
                    "[parent]": "[child]"
                }
            }
        }
    }
}
```
- 索引父文档
```
PUT /<index>/_doc/<ID>
{
    ……
    "[关联名称]": {
        "name": "[parent]"
    }
}
```
- 索引子文档
```
PUT /<index>/_doc/<ID>?routing=[父文档ID：确保和父文档索引到相同的分片]
{
    ……
    "[关联名称]": {
        "name": "[child]",
        "parent": "[父文档ID]"
    }
}
```
- 查询
```
// 获取父文档
GET /<index>/_doc/<parent_ID>
// 获取子文档，必须制定routing
GET /<index>/_doc/<child_id>?routing=<parent_ID>
// 查询父文档及其子文档
POST /<index>/_search
{
    "query": {
        "parent_id": {
            "type": "[child]",
            "id": "[父文档ID]"
        }
    }
}
// 查询子文档的其父文档
POST /<index>/_search
{
    "query": {
        "has_child": {
            "type": "[child]",
            "query": {……}
        }
    }
}
// 查询父文档的其子文档
POST /<index>/_search
{
    "query": {
        "has_parent": {
            "type": "[parent]",
            "query": {……}
        }
    }
}
```
- 更新子文档
```
PUT /<index>/_doc/<child_id>?routing=<parent_ID>
{
    ……
}
```

#### 嵌套对象 & 父子文档
对比|嵌套对象|父子文档
-|-|-
优点|文档文处在一起，读取性能高|父子文档可以独立更新
缺点|更像嵌套的子文档是，需要更新整个文档|需要额外的内存维护关系，读取性能相对差
适用场景|子文档更新频率低|子文档更新频繁

#### Update by Query
在现有的索引上进行重建索引
```
PUT /<index>/_update_by_query
{}
```
#### reindex
在其他索引上重建索引（在新的索引上使用新的Mapping）
```
PUT _reindex
{
    "source": {
        "index": "[旧索引]"
    },
    "dest": {
        "index": "[新索引]"
    }
}
```
1. 修改主分片
2. 修改Mapping中字段的类型
3. 集群内/跨集群数据迁移

#### Ingest Pipeline
1. Split Processor
2. Remove / Rename Processor
3. Append
4. Convert
5. Date / Json
6. Date Index Name Processor：将数据分配到指定时间格式的索引中
7. Fial Processor：一旦出现错误，改pipeline指定的错误信息返回给用户
8. Foreach Process
9. Grok Processor：日志的日期格式切割
10. Gsub / Join / Split：字符串替换 / 数组装字符串 / 字符串装数组
11. Lowercase / Upcase

对比|Logstash|Ingest Node
-|-|-
数据输入与输出|支持不同的数据源读取，并写入不同的数据源|支持从ES Rest API获取数据，并写入ES
数据缓冲|实现了简单的数据队列，支持重写|不支持缓冲
数据处理|支持大量的插件，也支持定制开发|内置的插件，可以开发Plugin进行扩展（Plugin需要重启）
配置和使用|增加了一定的架构复杂度|不需额外部署
```
// 测试
POST _ingest/pipeline/_simulate
{
    "pipeline": {
        "description": "[描述]",
        "prodessors": [ {
            "[process]": {
                "field": "[要处理的字段]",
                "[process参数]": "[值]"
            }
        } ]
    },
    "docs": {……}
}
// 添加pipeline
PUT _ingest/pipeline/<pipeline_name>
{
    "description": "[描述]",
    "prodessors": [ {
        "[process]": {
            "field": "[要处理的字段]",
            "[process参数]": "[值]"
        }
    } ]
}
// 查看pipeline
GET _ingest/pipeline/<pipeline_name>
// 测试pipeline
POST _ingest/pipeline/<pipeline_name>/_simulate
{
    "docs": {……}
}
// 使用pipeline索引数据
PUT /<index>/_doc/<ID>?pipeline=<pipeline_name>
{
    ……
}
// 使用updata_by_query用pipeline更新索引数据
POST /<index>/_update_by_query?pipeline=<pipeline_name>
{
    "query": {……}
}
```
#### Painless Script
1. 字段加工处理
2. 对返回字段进行计算
3. 对文档算分进行处理

上下文|语法
-|-
Ingestion|ctx.field_name
Update|ctx._source.field_name
Search & Aggregation|doc["field_name"]
```
"pipeline": {
    "description": "[描述]",
    "prodessors": [ {
        "script": {
            "source": "……”
        }
    } ]
}
// 保存script
PUT /_scripts/<script_name>
{
    "script": {
        "lang": "painless",
        "source": {……}
    }
}
// 使用脚本更新
POST /<index>/_update/<ID>
{
    "script": {
        "id": "<script_name>",
        "params": {
            "[参数名]": "[参数值]"
        }
    }
}
// 使用脚本查询
GET /<index>/_search
{
    "script_fields": {
        "[查询名称]": {
            "script": {……}
        }
    }
}
```
### 数据建模
#### 字段建模
```
graph LR
A[字段类型] --> B[是否需要搜索及分词]
B --> C[是否需要聚合及排序]
C --> D[是否需要额外的存储]
```
- 字段类型
1. Text：用于全文本搜索，会被分词，默认不支持聚合及排序，需要设置fielddata为true
2. Keyword：用于ID，枚举及不需要分词的文本，适用于filter（精确匹配）、排序、聚合
3. 多字段类型：默认为text设计一个keyword类型
4. 对结构化数据设置一个合适的数据类型

- 搜索建模
1. 不需要检索、排序、聚合分析：设置Enable为false
2. 不需要搜索：把index设置成false

- 聚合和排序
1. 不需要检索、排序、聚合分析：设置Enable为false
2. 不需要排序、聚合分析：设置doc_values / fielddata设置成false
3. 频繁更新，频繁进行聚合查询的字段：将eager_global_ordinals设置为true

- 额外存储
1. 需要额外存储：设置store为true（保存原始内容），结合_source的enabled设置为false使用
2. disable _source: 无法看到_source字段，无法做Reindex，无法做update，一般用于指标型数据

#### 建模最佳实践
- 如何处理关联关系
1. 优先考虑Object：对数据进行Denormalizattion
2. 考虑使用Nested：在数据包含多值对象，同时有查询需求
3. 使用Parent/Child：在关联文档更新非常频繁时

- 避免过多的字段
1. 不好维护：默认最大字段数1000，可通过`index.mapping.total_field.limt`设置
2. Mapping信息保存在Cluster State中，数据量过大，对集群性能会有影响
3. 删除会修改字段需要Reindex

- 避免正则查询
1. 属于term查询，性能不好，特别是通配符在前面时
2. 解决办法是拆分字段

- 尽量避免空置，容易引起聚合结果不准
- 为索引的Mapping文件键入Meta信息，并上传版本管理工具进行版本管理
```
PUT /<index>
{
    "mapping": {
        "_meta": {
            "[KEY]":"[VALUE]"
        }
    }
}
```
### 总结
- 结构化与非结构化搜索
1. Term查询：不会对查询字段进行分词
2. 全文本Match搜索：在text字段上进行查询会进行分词，在keyword上查询会转成Term查询
3. 对于需要进行精确匹配或需要进行聚合分析的字段，需要将字段类型设置成keyword

- Query Context & Filter Context
1. Filter Context可以避免算分，并且能够利用缓存
2. Bool查询中的Filter与Must Not都是Filter Context

- 搜索算分  
TF-IDF算法的概念一级Boosting权重
- 单字符串多字段查询：mulit-match  
1. Best_Field：返回单字段上算分最高的结果
2. Most_Fields：对算分结果进行相加后返回最高的
3. Cross_Field：混合，希望尽量多的找到匹配

- 提高搜索的相关性
1. 多语言：设置不同的字段和不同的分词器提升搜索的效果
2. Search Template分离代码逻辑和搜索DSL

- 聚合分析
1. Bucket：分桶（group by）
2. Metric：单值，sum、avg等
3. Pipeline：多次聚合

- 分页
1. From & Size
2. Search After：不支持向前反野
3. Scroll API：新插入数据查询不到
4. 避免深度分页，可以使用Scroll API

- 分布式存储
1. 分片的Hash算法，和routing
2. Segment：当前尚未刷盘的
3. Transaction Log：操作日志，立即刷盘
4. Refresh：将Segment刷盘
5. Merge：合并Segment，并清楚已经删除的数据
6. 相关性算分IDF是基于分片的，可以通过在聚合查询时增加shard_size增加算分的精准度

- 数据建模
1. 关系处理
2. Index Template / Dynamic Template：
3. Ingest Node：数据处理
4. Update By Query / Reindex：索引重建
5. Index Alias

## 六、安全
#### 保护Rest API
1. 启动X-Pack的security组件
```
xpack.security.enabled=true
xpack.security.transport.ssl.enabled: true
```
2. 设置密码
```
bin/elasticsearch-setup-passwords interactive
```
3. 配置Kibana
```
elasticsearch.username: "kibana_system"
elasticsearch.password: "<密码>"
```
#### 保护集群内数据安全
1. 创建CA
```
bin/elasticsearch-certutil ca
```
2. 签发证书
```
bin/elasticsearch-certutil cert --ca <生成的CA>
```
3. 配置ElasticSearch
```
xpack.security.transport.ssl.verification_mode: certificate
xpack.security.transport.ssl.keystore.path: <证书路径，一般放在certs/elastic-certificates.p12>
xpack.security.transport.ssl.truststore.path: <证书路径，一般放在certs/elastic-certificates.p12>
```
#### ES使用HTTPS
1. 配置ElasticSearch
```
xpack.security.http.ssl.enabled: true
xpack.security.http.ssl.keystore.path: <证书路径，一般放在certs/elastic-certificates.p12>
xpack.security.http.ssl.truststore.path: <证书路径，一般放在certs/elastic-certificates.p12>
```
2. 创建pem证书（使用之前生成的ES的证书）
```
openssl pkcs12 -in elastic-certificates.p12 -cacerts -nokeys -out elastic-ca.pem
```
2. 配置Kibana
```
elasticsearch.hosts:["https://<ip>:<port>"]
elasticsearch.ssl.verificationMode: certificate
elasticsearch.ssl.certificateAuthorities: [<pem证书路径>]
```
#### Kibana使用HTTPS
1. 生成pem压缩包
```
bin/elasticsearch-certutil ca --pem
```
2. 解压压缩包，得到ca.crt和ca.key
```
unzip <生成的ZIP压缩包>
```
3. 配置kibana
```
server.ssl.enabled: true
server.ssl.certificate: <ca.crt路径>
server.ssl.key: <ca.key路径>
```
## 七、水平扩展
### 节点类型
节点类型|参数|默认值
-|-|-
master eligible|node.master|true
data|node.data|true
ingest|node.ingest|true
coordinating only||设置node.master、node.data、node.ingest为false
machine learning|node.ml|true（需要启动X-Pack）

- Dedicated master eligible nodes  
1. 负责集群状态（Cluster State）的管理
2. 使用低配置的CPU、RAM和磁盘

- Dedicated data nodes  
1. 负责数据存储及客户端请求
2. 使用高配置的CPU、RAM和磁盘

- Dedicated ingest node  
1. 负责数据处理
2. 使用高配置CPU、中配置的RAM和低配置的磁盘

- Dedicated coordinating only
1. 起到Load Balance和降低Master和Date Nodes负载的作用
2. 对搜索结果进行Grather和Reduce
3. 使用中/高配置的CPU，中/高配置的RAM和第配置的磁盘

#### 部署
```
graph LR
K[浏览器访问kibana] --> C[Coorinating & Kibana]
A[application] --> LB1[写LB]
LB1 --> C
M[Master] --> C
M --> D[Data]
M --> I
LB2 --> I[Ingest]
A --> LB2[读LB]
C --> D
I --> D
```

#### Hot & Warm
1. 标记阶段（Tagging）
```
node.attr.<key>=<value>
```
2. 配置索引到相应的Node
```
PUT /<index>
PUT /<index>/_setting
{
    "settings": {
        ……
        "index.routing.allocation.require.<key>": "<value>"
    }
}
```
在数据较新的时候，配置Key-Value到Hot，相反配置Key-Value到Warm

#### Rack Awareness
将分片发送到不同的节点
1. 标记阶段（Tagging）
```
node.attr.<key>=<value>
```
2. 设定Rack Awareness
```
// 设置Rack Awareness
PUT /_cluster/setting
{
    "persistent": {
        //通过可以进行Rack Awareness
        "cluster.routing.allocation.awareness.attributes": "<key>",
        //指定必须把主/副本分片放在不同的value指定的机器
        "cluster.routing.allocation.awareness.force.zone.values": "<value>, <value>"
    }
}
// 设置索引Mapping，副本数>1
PUT /<index>
{
    "setting": {
        "number_of_shards": 2,
        "number_of_replicas": 1
    }
}
```
- 其他一些标记

配置|分片索引到节点，节点属性的规则
-|-
index.routing.allocation.include.<key>|至少包含一个值
index.routing.allocation.exclude.<key>|不包含任何值
index.routing.allocation.require.<key>|必须包含所有值

#### 分片
1. 日志类单分片不要大于50G，搜索内单分片不要大于20G
2. 通过`index.routing.allocation.total_shards_pre_node`设置每个索引在单个节点上的分片数
3. 通过`cluster.routing.allocation.total_shards_pre_node`单个节点最大的分片数，默认`-1`不限制

#### 容量规划
- 基础数据
1. 机器配置
2. 节点数、主/副分片数
3. 单文档大小、文档总数、索引数据量
4. 文档的复杂度、如何写入、读取（搜索和聚合）

- 性能要求
1. 写入和查询的吞吐量
2. 最大允许的响应时间
3. 数据的格式以及Mapping的设定
4. 可能的查询、聚合方式

对比|搜索类|日志类|
-|-|-
数据增长速度|增长快</br>可使用Warm Node做数据老化处理|增长相对较慢|
建议硬件配置|使用SSD磁盘</br>内存磁盘比1:16？|写入和查询并发较低时可以使用机械硬盘</br>内存磁盘比1:48-96？
单节点数据量|控制在2T内，不要超过5T|控制在2T内，不要超过5T
单节点内存|JVM内存为机器内存一半，同时不要超过32G

## 八、运维
#### 生产部署
- JVM
1. Xms和Xmx设置成同一个值，避免Heap Resize时引发停顿
2. Xmx不要超过物理内存的50%，单节点不要超过32G
3. 使用server模式，关闭JVM Swapping

- 配置信息
1. Transient Setting：重启后失效
```
{
    "transient": {
        "<key>": "<value>"
    }
}
```
2. Persistent Setting：重启后保留
```
{
    "persistent": {
        "<key>": "<value>"
    }
}
```
3. Command-line Setting：命令行设置
`-E <key>=<value>`
4. Config-file Setting
`elasticsearch.yml`

- 硬件配置
1. 建议使用中等配置的机器
2. 不建议在一台服务器上运行多个实例

- 网络配置
1. 不要使用WAN，单集群不要夸数据中心
2. 多网卡情况下，将Transport和HTTP绑定到不同的网卡并设置防火墙
3. 根据需要，配置Coordinating Node和Ingest Node的负载均衡

- 内存/磁盘
1. 搜索类：1:16；日之类：1:48-96
2. 磁盘做一定的预留

- 存储
1. 推荐使用SSD，Warm Node可以使用机械磁盘，但是要设置`index.merge.scheduler.max_thread_count: 1`关闭Concurrent Merges
2. ES自带HA无需使用RAID
3. 可以指定多个`path.data`，分别到多个磁盘

- Throttles限流：避免多任务对集群的影响
1. 设置recovery：`cluster.routing.allocation.node_concurrent_recoveries: 2`
2. 设置relocation：`cluster.routing.allocation.cluster_concurrent_recoveries: 2`

#### 监控
- 日志记录
```
PUT /<index>
{
    "settings": {
        "index.search.slowlog.threshold": {
            "query.warn": "10s",
            "query.info": "4s",
            "query.debog": "2s",
            "query.trace": "0s",
            "fetch.warn": "1s",
            "fetch.info": "500ms",
            "fetch.debog": "300ms",
            "fetch.trace": "0s",
        }
    }
}
```
- 集群健康度
1. 红色：存在主分片没有分配
2. 黄色：存在副本分片没有分片
3. 绿色：主/副本分片都正常分配
```
// 查看状态
GET /_cluster/helth
GET /_cluster/helth/<index>
GET /_cluster/helth?level=indix
GET /_cluster/helth?level=shards
// 查看原因
GET /_cluster/allocation/explain

// 移动索引
POST /_cluster/reroute
{
    "commands": {
        "move": {
            "index": "[要移动的索引]",
            "shard": 0.
            "from_node": "[原节点]",
            "to_node": "[新节点]"
        }
    }
}
```
#### 性能优化
- mapping设置
1. 不需要搜索：index设置成false
2. 不需要算分：Norms设置成false
3. 不对字符串使用dynamic mapping
4. 指标类数据，关闭_source

- Refresh设置
1. 增大refresh_interval的值降低refresh频率
2. 增大`indices.memory.index_bufer_size`的值，防止缓存写满而被动刷盘
3. **问题**：会降低搜索的实时性

- translog设置
1. 配置`index.translog.durability`为`async`，实现异步写入
2. 增大`index.translog.sync_interval`的值，降低刷盘频率
3. 增加`index.translog.flush_threshod_size`的值，防止缓存写满而被动刷盘
4. **问题**：可能造成数据丢失

- 分片设置
1. 副本分片在写入时设置成0，待写完后再增加
2. 合理设置分片数，确保分片均匀分配到节点上，可通过把`index.routing.allocation.total_per_node`设置成大于`(主分片数+副本分片数)/节点个数`的值来限制每个索引在每个节点上的主分片数
3. 避免Over Sharding
4. 对基于时间序列的索引，可以将老数据（不在进行写入）做只读操作，并进行force merge，减少segment数量

- Bulk
1. 数据量不要太大，一般控制在5-15M
2. 请求超时建议在60s以上
3. 客户端要负载到不同节点写入
4. 服务端使用线程池处理【线程池设置最佳实践：线程池大小、队列大小等】

- Query
1. 多可能的查询找保存的时候做一些先行的计算，将结果保存找ES中
2. 使用Filter Context减少不必要的算分并利用缓存
3. 避免对term查询使用前缀通配符
4. 在聚合查询中使用Query减少查询的数据量，减少内存开销
5. 需要使用不同的Query Scope时，可以利用Filter Bucket
6. 避免嵌套查询：嵌套查询需要很大的堆内存来完成，如果业务需要可以增加硬件来进行扩容，同时需要设置Circuit Break和`search.max_buckets`的值来了避免嵌套影响整个集群

- Circuit Break（断路器）：防止不和你的操作，引发集群OOM问题
```
//查看系统中熔断的状况
GET /_nodes/stats/breaker
```
1. Parent circuit breaker：设置所有得熔断器可以使用的内存的总量
```
indices.breaker.total.use_real_memory
```
2. Fielddata circuit breaker：加载fielddata所需要的内存
3. Request circuit breaker：防止每个请求级数据结构操作一定的内存（如聚合运算需要的内存）
4. In Flight circuit breaker：Request中的断路器
5. Accounting request circuit breaker：请求结束后不能释放的对象所占的内存

#### 压力测试
- 基于HTTP API
1. Load Runner
2. [JMeter](https://jmeter.apache.org/)
3. [Gatling](https://gatling.io/)

- 专门为ES设计
1. ES Pref & Elasticsearch-strees-test
2. [Elastic Rally](https://github.com/elastic/rally)

#### Rally
- 安装
```
// Python 3.4+、Pip3、JDK8、git 1.9+
pip3 install esrally
esrally configure
```
- 运行
```
esrally --distrbution-version=<ES版本>
或使用少量（1000条）测试数据
esrally --distrbution-version=<ES版本> --test-mode
```
- esrally基本概念
1. Tournament：定义测试目标，有多个race-测试组成
```
esrally list race
```
2. [Track](https://github.com/elastic/rally-tracks)：测试数据和测试场景策略
```
esrally list tracks
```
3. [Car](https://esrally.readthedocs.io/en/latest/car.html)：执行测试方案-不同配置的es实例
4.  Award：测试结果和报告

#### 段（segment）合并
```
POST /<index>/_forcemerge?v
POST /<index>/_forcemerge?max_num_segments=<num>
```
1. 调大Refresh Interval至分钟级别，增加`indices.memroy.index_buffer_size`的值（默认为10%），同时降低文档更新的操作避免生成过多的segment
2. 调整`index.merger.policy.segments_per_tier`的值（默认是10）该值越小需要合并的操作就越多；调整`index.merge.policy.max_meged_segment`的值（默认是5G），合并后segment的大于该值，就不会再参与后续的合并操作了，这两个值配合可以防止过大的segment参与Merage而节约资源（最终会形成多个segment）
3. 在index不在有写操作是，设置其为read_only提升查询速度，减少内存开销

#### 缓存
- Node Query Cache
1. 每一个节点都有Node Query Cache，有所用的Shard共享，值缓存Filter Context相关的内容，使用LRU淘汰算法
2. 配置参数（需要配置在每个Data Node上）
```
Node Level：indices.queries.cache.size: "10%"
Index Level: index.queries.cache.enabled: true
```
3. 发生Segment合并便会失效

- Shard Query Cache（Cache Query的结果）
1. 缓存每个分片上的查询结果，且只缓存设置了`size=0`的查询。不会缓存hits，但是会缓存Aggregations和Suggestions，使用LRU算法，以查询字符串的整个JSON作为Key（尽量保证查询字段顺序相同）
2. 配置参数（需要配置在每个Data Node的yml文件中）
```
indices.queries.cache.size: "10%"
```
3. 分片Refresh时便会时效

- Fielddata Cache
1. 除text类型外，默认都采用doc_values，Aggregation的Global ordinals也保存在Fielddata cache中，text类型字段可以打开fileddata才能进行聚合和排序（不建议）
2. 配置
```
indices.fielddata.cache.size，需要控制大小，避免产生GC
```
3. 发生Segment合并便会失效
4. Fileddata Cache的构建是一个相对比较重的操作，ES不会对主动释放，因此应该设置得相对保守些，如果业务确实有需要，也可以通过增加节点扩容的方式进行解决

- 查看内存状况API
```
GET -cat/nodes?v
GET _nodes/stats/indices?pretty
GET _cat/nodes?v&h=name,queryCacheMemory,queryCacheEvictions,requestCacheMemory,requestCacheHitCount,request_cache.miss_count
GET _cat/nodes?h=name,port,segments.memory,segments.index_writer_memory,fielddata.memory_size,query_cache.memory_size,request_cache.memory_size&v
```

## 九、索引
- Open / Close Index：索引关闭后无法进行读写，但是索引数据不会被删除
```
POST /<index>/_close
POST /<index>/_open
```
- Shrink Index：可以将索引的主分片数收缩到较小的值
1. 分片必须只读
2. 所有分片必须在同一节点上
3. 集群必须为green状态
4. 旧分片数必须是新分片数的整数倍
```
POST /<source_index>/_shrink/<target_index>
{
    "settings": {
        "index.number_of_replicas": "[新副本数]",
        "index.number_of_shards": "[新分片数]",
        "index.codes": "[best_compression]"
    },
    "aliases": {
        "aliases_inddces": {}
    }
}
```
- Split Index：可以扩大主分片数
1. 分片必须只读
2. 新分片数必须是旧分片数的整数倍
```
POST /<source_index>/_split/<target_index>
{
    "settings": {
        "index.number_of_shards": "[新分片数]"
    },
    "aliases": {
        "aliases_inddces": {}
    }
}
```
- Rellover Index：索引尺寸或时间超过一定值后，创建新的（类似于日志文件）
1. 不会自动触发，一般会结合Index Lifecycle Management Policies结合使用
```
POST /<index_alias>/_rollover
{
    "conditions": {
        "max_age": "[最大时间]",
        "max_docs": [最大文档数],
        "max_size": "[最大空间]",
    }
}
```
- Rollup Index：对数据进行处理后，重新写入，减少数据量

#### Index Lifecycle Management
- Policy
- Phase
- Action
```
//1. 设置Index Lifecycle执行时间间隔
PUT _cluster/setting
{
    "persistent": {
        "indices.lifecycle.poll_interval": "[时间，默认10m]"
    }
}
//2. 设置Index Lifecycle Policy
PUT /_ilm/policy/<policy_name>
{
    "policy": {
        "phase": {
            "[phase_name1]": {……},
            "[phase_name2]": {……},
            "[phase_name3]": {……},
            "[phase_name4]": {……}
        }
    }
}
//3. 设置索引模板（可略过）
PUT /_template/<template_name>
{
    "index_patterns": ["[index_patten]"],
    "settings": {
        "index": {
            "lifecycle": {
                "name": "[policy_name]",
                "rollover_alias": "[index_alias_name]"
            }
        }
    }
}
//4. 创建索引
PUT /<index>
{
    "settings": {
        "index.lifecycle.name": "[policy_name]",
        "index.lifecycle.rollover_alias": "[index_alias_name]"
    },
    "aliases": {
        "[index_alias_name]": {
            "is_write_index": true
        }
    }
}
//5. 操作索引alias写入数据
POET /<index_alias_name>/_doc
{
    ……
}
```
## 十、LogStash & Beats & Kibana

## 备份与恢复
```
# 1. 配置elasticsearch.yml
path.repo: ["路径"]
# 2. 创建repository
PUT /_snapshot/<repository_name>
{
    "type": fs,
    "settings": {
        "location": "path.repo设置的路径",
        "compress": true/false
    }
}
# 3. 创建snapshot
PUT /_snapshot/<repository_name>/<snapshot_name>?wait_for_completion=true
{
    "indices": "要打包的索引的名字"
}

# 查看repository下所有snapshot
GET /_snapshot/<repository_name>/_all
# 删除repository
DELETE /_snapshot/<repository_name>
# 删除snapshot
DELETE /_snapshot/<repository_name>/<snapshot_name>
# 恢复快照
POST /_snapshot/<repository_name>/<snapshot_name>/_restore
{
    "indices": "要恢复的索引的名字"
}
```

---
###### 如何购买

1. 扫描二维码
	<div align="center">
		<a href="https://time.geekbang.org/course/intro/100030501?code=T9tIKdvgIa6Yt7dyq6gtgeBgtwpjBhOoWMurD-82OA8%3D">
			<img src="/img/geekbang/ES.jpg" width = "300" height = "300" alt="图片名称" style="display: inline-block"/>
		</a>
	</div>
2. 点击链接  
[极客时间-直连](https://time.geekbang.org/course/intro/100030501?code=T9tIKdvgIa6Yt7dyq6gtgeBgtwpjBhOoWMurD-82OA8%3D)  